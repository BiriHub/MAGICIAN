{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7657a2",
   "metadata": {},
   "source": [
    "# MAGICIAN: Mining and gAmblingG servIces sCrapIng and ANalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fec77",
   "metadata": {},
   "source": [
    "## Setting up the environment for the project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0471610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraris \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987235d5",
   "metadata": {},
   "source": [
    "### Configuration for the scraping process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.walletexplorer.com\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e258d",
   "metadata": {},
   "source": [
    "### Check the robot.txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "# Parse the robots.txt file \n",
    "rp = urllib.robotparser.RobotFileParser(BASE_URL + \"/robots.txt\")\n",
    "rp.read()\n",
    "\n",
    "if not rp.mtime() or not rp.can_fetch(\"*\", BASE_URL + \"/robots.txt\"):\n",
    "   print(\"robots.txt could not be read or is not present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a943e",
   "metadata": {},
   "source": [
    "# Extracting DeepBit.net and DiceOnCrack.com wallet addresses\n",
    "\n",
    "I create a small pipeline to scrape the walletexplorer website and then extract the wallet addresses from the pages of the two websites by using two functions.\n",
    "The idea is to firstly scrape the main page of the walletexplorer website and then for each service, scrape the wallet addresses page and extract the addresses from the table. For each request to the walletexplorer website, before proceding I wait 5 seconds to avoid overwhelming the server otherwise I would get ban temporarily from the website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walletexplorer_page():\n",
    "    \"\"\" \n",
    "    Retrieve the main page of WalletExplorer\n",
    "\n",
    "    This function sends a GET request to the WalletExplorer main page using the specified headers and before sending the request it waits 5 seconds before making the request to avoid overwhelming the server.\n",
    "    Returns the response object if successful, otherwise returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server\n",
    "        print(\"Accessing WalletExplorer main page...\")\n",
    "        main_walletexplore_page = requests.get(BASE_URL, headers=HEADERS)\n",
    "        main_walletexplore_page.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error while accessing WalletExplorer:\", e)\n",
    "        return None\n",
    "    return main_walletexplore_page\n",
    "\n",
    "def scraping_wallet_address(html_page,service_name):\n",
    "    \"\"\"\n",
    "    Scrape the wallet addresses of a specific service from WalletExplorer.\n",
    "\n",
    "    This function takes the HTML page of WalletExplorer and the name of the service as input.\n",
    "    Firstly, it accesses the page of the service by searching for its name, secondly, it navigates to the wallet addresses page and in the end, it extracts the wallet addresses from the table on that page.\n",
    "    Returns a pandas Series of wallet addresses (hashes) or an empty Series if none are found.\n",
    "\n",
    "    Parameters:\n",
    "    - html_page: str, the HTML page of WalletExplorer.\n",
    "    - service_name: str, the name of the service to search for.\n",
    "    Returns:\n",
    "    - pd.Series: a pandas Series containing the wallet addresses (hashes) of the service, or an empty Series if none are found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open search page regarding 'service_name' and open the wallet addresses page\n",
    "    try: \n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server\n",
    "        print(f\"Accessing the search page for '{service_name}'...\")\n",
    "\n",
    "        search_page = requests.get(html_page, headers=HEADERS, params={'wallet' :service_name}, timeout=60)\n",
    "        search_page.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error while accessing the search page:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Scrape the search results and extract the wallet addresses of 'service_name'\n",
    "    soup = BeautifulSoup(search_page.text, 'html.parser')\n",
    "\n",
    "    # Find the url of the wallet addresses page\n",
    "    span = soup.find('span', {'class': 'showother'})\n",
    "\n",
    "    wallet_link = span.find('a').get('href')\n",
    "    wallets_url = BASE_URL + wallet_link # create the full URL for the wallet addresses page\n",
    "\n",
    "    # Access the wallet addresses page using the URL found in the search results\n",
    "    try:\n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server        \n",
    "        wallet_addr_page = requests.get(wallets_url, headers=HEADERS)\n",
    "        wallet_addr_page.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while accessing the wallets page: {e}\")\n",
    "        return None     \n",
    "\n",
    "    # Scrape the wallet addresses page extracting the information from the table\n",
    "    soup = BeautifulSoup(wallet_addr_page.text, 'html.parser')\n",
    "\n",
    "    # Save the wallet address of 'service_name'\n",
    "    tmp = []\n",
    "\n",
    "    # Find the table containing the wallet addresses\n",
    "    wallet_table = soup.find('table')\n",
    "\n",
    "    for row in wallet_table.find_all('tr'):\n",
    "        col = row.find('td')\n",
    "        if col and col.find('a', href=True):\n",
    "            addr = col.find('a')\n",
    "            tmp.append(addr.text.strip())\n",
    "    \n",
    "    return pd.Series(tmp, name='hash') if tmp else pd.Series([], name='hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abacc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepbit_service = \"DeepBit.net\"\n",
    "diceoncrack_service = \"DiceOnCrack.com\"\n",
    "\n",
    "# Open the main page of WalletExplorer\n",
    "main_walletexplore_page = get_walletexplorer_page()\n",
    "# Check if the main page was retrieved successfully\n",
    "if main_walletexplore_page is None:\n",
    "    print(\"Failed to retrieve the main WalletExplorer page.\")\n",
    "else:\n",
    "    # Search the form in the page\n",
    "    soup = BeautifulSoup(main_walletexplore_page.text, 'html.parser')\n",
    "    search_form = soup.find('form', {'class':'main'})\n",
    "    action_form = search_form.get('action')\n",
    "\n",
    "    # Create the target URL for the search form\n",
    "    if action_form.startswith('/'):\n",
    "        target_url = BASE_URL + action_form\n",
    "    else:\n",
    "        target_url = BASE_URL+ '/'+action_form\n",
    "\n",
    "\n",
    "    # Get the wallet addresses for DeepBit.net\n",
    "    print(f\"Searching for wallet addresses of {deepbit_service}...\")\n",
    "    deepbit_wallet_addresses = scraping_wallet_address(target_url, deepbit_service)\n",
    "    if deepbit_wallet_addresses is None:\n",
    "        print(f\"Failed to retrieve wallet addresses for {deepbit_service}.\\n\")\n",
    "    else :\n",
    "        print(f\"Wallet addresses for {deepbit_service} have been successfully retrieved.\\n\")\n",
    "    # Get the wallet addresses for DiceOnCrack.com\n",
    "    print(f\"Searching for wallet addresses of {diceoncrack_service}...\")\n",
    "    diceoncrack_wallet_addresses = scraping_wallet_address(target_url, diceoncrack_service)\n",
    "    if diceoncrack_wallet_addresses is None:\n",
    "        print(f\"Failed to retrieve wallet addresses for {diceoncrack_service}.\\n\")\n",
    "    else :\n",
    "        print(f\"Wallet addresses for {diceoncrack_service} have been successfully retrieved.\\n\")\n",
    "#Print the results\n",
    "print(f\"DeepBit.net wallet addresses: {deepbit_wallet_addresses}\")\n",
    "print(f\"DiceOnCrack.com wallet addresses: {diceoncrack_wallet_addresses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc12ff",
   "metadata": {},
   "source": [
    "#### Load datasets for the next analysis regarding the service DeepBit.net and DiceOnCrack.com\n",
    "\n",
    "Given that the great size of the datasets, I decided to read the csv files only one time in order to avoid to read them multiple times during the analysis. I used Pyarrow engine to read the csv files due to its speed and efficiency in handling large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "transactions = pd.read_csv('transactions.csv', engine='pyarrow')\n",
    "transactions.columns = ['timestamp', 'blockId', 'txId', 'isCoinbase', 'fee']\n",
    "\n",
    "outputs = pd.read_csv('outputs.csv', engine='pyarrow')\n",
    "outputs.columns = ['txId', 'position', 'addressId', 'amount', 'scripttype']\n",
    "\n",
    "inputs = pd.read_csv('inputs.csv', engine='pyarrow')\n",
    "inputs.columns = ['txId', 'prevTxId', 'prevTxpos']\n",
    "\n",
    "mapping = pd.read_csv('mapping.csv', engine='pyarrow')\n",
    "mapping.columns = ['hash', 'addressId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe0786",
   "metadata": {},
   "source": [
    "# Deepbit.net's mining pool analysis\n",
    "\n",
    "Subsequently, I will analyze the dataset of the Deepbit.net mining pool. The analysis will focus on the distribution of the mined blocks, the distribution of the received fees and the computation of the UTXO (Unspent Transaction Output) regarding the service Deepbit.net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db954734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_wallet_addresses(wallet_addresses,mapping_dt):\n",
    "    \"\"\"\n",
    "    Map the wallet addresses to their corresponding addressId in the mapping dataset.\n",
    "    Returns a Series of addressId found or an empty Series if none found.\n",
    "\n",
    "    Parameters:\n",
    "    - wallet_addresses: pd.Series, a Series of wallet addresses (hashes) to be mapped.\n",
    "    - mapping_dt: pd.DataFrame, a DataFrame containing the mapping of wallet addresses to addressId.\n",
    "    Returns:\n",
    "    - pd.Series: a pandas Series containing the addressId corresponding to the wallet addresses, or an empty Series if none are found.\n",
    "    \"\"\"\n",
    "    mapped_addresses = mapping_dt.merge(wallet_addresses,on ='hash')['addressId']\n",
    "    if mapped_addresses.empty:\n",
    "        return pd.Series([], name='addressId')\n",
    "    \n",
    "    return mapped_addresses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Identify the mapping between wallet addresses on the dataset\n",
    "deepbit_mapped_addresses = map_wallet_addresses(deepbit_wallet_addresses,mapping)\n",
    "\n",
    "print(deepbit_mapped_addresses.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd00271",
   "metadata": {},
   "source": [
    "## 1) Deepbit.net's mined block distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9212c",
   "metadata": {},
   "source": [
    "Firslty, it needs to extract the transactions of DeepBit.net from the datasets, which are the transactions that have at least one input or output address of DeepBit.net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wallet_transactions(wallet_mapped_addresses, transactions, inputs, outputs):\n",
    "    \"\"\"\n",
    "    Finds all transactions related to the specified service addresses.\n",
    "    This function takes the mapped addresses of a service and the transactions, inputs, and outputs datasets and the retrieves all transactions that have at least one output address of the service.\n",
    "    It returns a DataFrame containing all transaction details related to the service.\n",
    "\n",
    "    Parameters:\n",
    "    - wallet_mapped_addresses: pd.Series, a Series of addressId corresponding to the wallet\n",
    "    addresses of the service.\n",
    "    - transactions: pd.DataFrame, a DataFrame containing transaction details.\n",
    "    - inputs: pd.DataFrame, a DataFrame containing input details of transactions.\n",
    "    - outputs: pd.DataFrame, a DataFrame containing output details of transactions.\n",
    "    Returns:\n",
    "    - pd.DataFrame: a DataFrame containing all transaction details related to the service.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all outputs information (address, amount, ...) regardin the service\n",
    "    diceoncrack_output_transactions = outputs.merge(wallet_mapped_addresses,on='addressId') \n",
    "    \n",
    "    # Find all inputs information (address, amount, ...) regarding the service and rename the columns to prepare for the merge between inputs and outputs dataframes\n",
    "    renamed_outputs= diceoncrack_output_transactions.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "    wallet_input_transactions = inputs.merge(renamed_outputs,on=['prevTxId', 'prevTxpos'])\n",
    "\n",
    "    # Make the union of all transactions that have at least one output address of the service\n",
    "    df_union = pd.concat([diceoncrack_output_transactions['txId'], wallet_input_transactions['txId']], axis=0,ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Merge the union of transactions with the transactions dataset to get all transaction details\n",
    "    wallet_transactions = transactions.merge(df_union,on='txId') \n",
    "\n",
    "    return wallet_transactions\n",
    "\n",
    "\n",
    "# Find the transactions related to Deepbit.net\n",
    "deepbit_transactions = find_wallet_transactions(deepbit_mapped_addresses, transactions, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f884570",
   "metadata": {},
   "source": [
    "### Find the distribution of the mined blocks by DeepBit.net in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403145c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the transaction patterns for the wallet addresses\n",
    "\n",
    "# 1. Select all coinbase transactions (mining rewards) from the transactions dataset and merge them with the outputs dataset in order to get their output information \n",
    "coinbase_transactions = transactions[transactions['isCoinbase'] == 1]['txId']\n",
    "merged_outputs_first_transact= outputs.merge(coinbase_transactions, on='txId')\n",
    "\n",
    "# 2. Find all outputs where Deepbit.net addresses appear (as outputs)\n",
    "deepbit_asoutput_transactions = outputs.merge(deepbit_mapped_addresses, on='addressId')\n",
    "tmp = deepbit_asoutput_transactions.merge(deepbit_transactions, on='txId')\n",
    "inputs_second_transaction = tmp.merge(inputs, on='txId')\n",
    "\n",
    "# Merge to find the base pattern: transactions where Deepbit.net spends outputs from mined blocks\n",
    "merged_outputs_first_transact.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'}, inplace=True)\n",
    "deepbit_mined_block_transact = inputs_second_transaction.merge(merged_outputs_first_transact, on=['prevTxId', 'prevTxpos']).drop_duplicates()[['txId', 'prevTxId', 'prevTxpos','timestamp']]\n",
    "\n",
    "# Lets prepare the data for Deepbit.net block distribution\n",
    "deepbit_block_distribution = deepbit_mined_block_transact.copy()\n",
    "deepbit_block_distribution['timestamp'] = pd.to_datetime(deepbit_block_distribution['timestamp'], unit='s')\n",
    "deepbit_block_distribution.set_index('timestamp', inplace=True)\n",
    "\n",
    "# 1. Daily count\n",
    "daily = deepbit_block_distribution.resample('D').size()\n",
    "# 2. Weekly count\n",
    "weekly = deepbit_block_distribution.resample('W').size()\n",
    "# 3. Monthly count\n",
    "monthly = deepbit_block_distribution.resample('ME').size()\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(3, 2, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of blocks mined by Deepbit.net', fontsize=16)\n",
    "\n",
    "# Line plots (left)\n",
    "axs[0, 0].plot(daily.index, daily.values, label='Daily', color='blue')\n",
    "axs[0, 0].set_title('Mined blocks - Daily')\n",
    "axs[0, 0].set_ylabel('Blocks')\n",
    "axs[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axs[1, 0].plot(weekly.index, weekly.values, label='Weekly', color='green')\n",
    "axs[1, 0].set_title('Mined blocks - Weekly')\n",
    "axs[1, 0].set_ylabel('Blocks')\n",
    "axs[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axs[2, 0].plot(monthly.index, monthly.values, label='Monthly', color='orange')\n",
    "axs[2, 0].set_title('Mined blocks - Monthly')\n",
    "axs[2, 0].set_ylabel('Blocks')\n",
    "axs[2, 0].set_xlabel('Date')\n",
    "axs[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar plots (right)\n",
    "axs[0, 1].bar(daily.index, daily.values, color='blue')\n",
    "axs[0, 1].set_title('Bar plot - Daily')\n",
    "axs[0, 1].set_ylabel('Blocks')\n",
    "axs[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axs[1, 1].bar(weekly.index, weekly.values, color='green', width=5)\n",
    "axs[1, 1].set_title('Bar plot - Weekly')\n",
    "axs[1, 1].set_ylabel('Blocks')\n",
    "axs[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axs[2, 1].bar(monthly.index, monthly.values, color='orange', width=20)\n",
    "axs[2, 1].set_title('Bar plot - Monthly')\n",
    "axs[2, 1].set_ylabel('Blocks')\n",
    "axs[2, 1].set_xlabel('Date')\n",
    "axs[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f9aeb",
   "metadata": {},
   "source": [
    "## 2) DeepBit.net's fee distribution\n",
    "\n",
    "Find how fee transactions are distributed to the DeepBit.net's mining pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify coinbase transactions information\n",
    "renamed_outputs = outputs.copy()\n",
    "renamed_outputs.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'}, inplace=True)\n",
    "fee_info= renamed_outputs.merge(deepbit_mined_block_transact, on=['prevTxId','prevTxpos']).merge(transactions[['txId', 'blockId', 'fee']], on='txId')\n",
    "\n",
    "# Convert timestamp to datetime and create a time series\n",
    "fee_info['timestamp'] = pd.to_datetime(fee_info['timestamp'], unit='s')\n",
    "fee_info.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Define the blockId of the first Bitcoin halving event.\n",
    "first_btc_halving_blockId = 210000\n",
    "# Convert the output amount from satoshis to BTC\n",
    "fee_info['amount'] = fee_info['amount'] / 1e8  \n",
    "\n",
    "# Calculate the expected block reward (50 BTC before halving, 25 BTC after) and compute the actual fee in BTC for each mined block transaction\n",
    "fee_subtract = (fee_info['blockId'] < first_btc_halving_blockId).map({True: 50, False: 25})\n",
    "fee_info['fee_btc'] = (fee_info['amount']+fee_info['fee'] - fee_subtract).round(8)\n",
    "\n",
    "\n",
    "# Aggregations\n",
    "daily_fee   = fee_info['fee_btc'].resample('D').sum()\n",
    "weekly_fee  = fee_info['fee_btc'].resample('W').sum()\n",
    "monthly_fee = fee_info['fee_btc'].resample('ME').sum()\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(3, 2, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of fees from Deepbit.net', fontsize=16)\n",
    "\n",
    "# Daily bar plot\n",
    "axs[0,0].bar(daily_fee.index, daily_fee.values)\n",
    "axs[0,0].set_title('Daily BTC fees')\n",
    "axs[0,0].set_xlabel('Date')\n",
    "axs[0,0].set_ylabel('Total fee (BTC)')\n",
    "axs[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly bar plot\n",
    "axs[1,0].bar(weekly_fee.index, weekly_fee.values, color='green', width=5)\n",
    "axs[1,0].set_title('Weekly BTC fees')\n",
    "axs[1,0].set_xlabel('Week (end of week)')\n",
    "axs[1,0].set_ylabel('Total fee (BTC)')\n",
    "axs[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly bar plot\n",
    "axs[2,0].bar(monthly_fee.index, monthly_fee.values, color='orange', width=20)\n",
    "axs[2,0].set_title('Monthly BTC fees')\n",
    "axs[2,0].set_xlabel('Month')\n",
    "axs[2,0].set_ylabel('Total fee (BTC)')\n",
    "axs[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily bar plot (log scale)\n",
    "axs[0,1].bar(daily_fee.index, daily_fee.values)\n",
    "axs[0,1].set_yscale('log')\n",
    "axs[0,1].set_title('Daily BTC fees (log scale)')\n",
    "axs[0,1].set_xlabel('Date')\n",
    "axs[0,1].set_ylabel('Total fee (BTC)')\n",
    "axs[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly bar plot (log scale)\n",
    "axs[1,1].bar(weekly_fee.index, weekly_fee.values, color='green', width=5)\n",
    "axs[1,1].set_yscale('log')\n",
    "axs[1,1].set_title('Weekly BTC fees (log scale)')\n",
    "axs[1,1].set_xlabel('Week (end of week)')\n",
    "axs[1,1].set_ylabel('Total fee (BTC)')\n",
    "axs[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly bar plot (log scale)\n",
    "axs[2,1].bar(monthly_fee.index, monthly_fee.values, color='orange', width=20)\n",
    "axs[2,1].set_yscale('log')\n",
    "axs[2,1].set_title('Monthly BTC fees (log scale)')\n",
    "axs[2,1].set_xlabel('Month')\n",
    "axs[2,1].set_ylabel('Total fee (BTC)')\n",
    "axs[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75a5b1",
   "metadata": {},
   "source": [
    "## 3) DeepBit.net's UTXO distribution\n",
    "\n",
    "Compute the UTXO (Unspent Transaction Output) distribution of the DeepBit.net for each month in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ce2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTXO are related to the unspent outputs of a transaction.\n",
    "\n",
    "# Merge DeepBit.net related transactions with outputs to get all outputs with their timestamps\n",
    "utxo_transactions = outputs.copy().merge(deepbit_transactions[['txId', 'timestamp']], on='txId')\n",
    "utxo_transactions.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'}, inplace=True)\n",
    "\n",
    "# Merge with inputs to check which outputs have been spent\n",
    "spent_utxo = utxo_transactions.merge(inputs[['prevTxId', 'prevTxpos']],on=['prevTxId', 'prevTxpos'], how='left', indicator='is_spent')\n",
    "\n",
    "# Identify unspent outputs (UTXO)\n",
    "unspent_utxo = spent_utxo[spent_utxo['is_spent'] == 'left_only'].drop(columns=['is_spent'])\n",
    "\n",
    "# Convert the amount from satoshis to BTC and the timestamp to datetime.\n",
    "unspent_utxo['amount'] = unspent_utxo['amount'] / 1e8 \n",
    "unspent_utxo['timestamp'] = pd.to_datetime(unspent_utxo['timestamp'], unit='s')\n",
    "\n",
    "# Create a range of month-end dates covering the period of the UTXOs.\n",
    "all_dates = pd.date_range(\n",
    "    start=unspent_utxo['timestamp'].min().to_period('M').to_timestamp(),\n",
    "    end=unspent_utxo['timestamp'].max(),\n",
    "    freq='ME'\n",
    ")\n",
    "# Adjust the end of the month to include the last second of the month (for example, 2011-11-31 23:59:59)\n",
    "all_dates = all_dates.floor('D') + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n",
    "\n",
    "# Calculate monthly UTXO\n",
    "# For each month, calculate the sum and count of UTXOs available before the end of that month.\n",
    "monthly_utxo =[]\n",
    "for month_end in all_dates:\n",
    "    before_month_end_utxo = unspent_utxo[unspent_utxo['timestamp'] < month_end]\n",
    "    sum_utxo_btc = before_month_end_utxo['amount'].sum()\n",
    "    utxo_count = before_month_end_utxo['amount'].count()\n",
    "    monthly_utxo.append([month_end, sum_utxo_btc, utxo_count])\n",
    "\n",
    "monthly_utxo_df = pd.DataFrame(monthly_utxo, columns=['month', 'utxo_btc', 'utxo_count'])\n",
    "\n",
    "# Visualize the monthly UTXO using bar plots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "fig.suptitle('Monthly distribution of DeepBit.net UTXOs', fontsize=16)\n",
    "\n",
    "# Bar plot UTXO BTC\n",
    "axs[0].bar(monthly_utxo_df['month'], monthly_utxo_df['utxo_btc'], color='blue', width=20, align='center')\n",
    "axs[0].set_ylabel('UTXO (BTC)')\n",
    "axs[0].set_title('Monthly sum of UTXOs (BTC)')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar plot UTXO Count\n",
    "axs[1].bar(monthly_utxo_df['month'], monthly_utxo_df['utxo_count'], color='orange', width=20, align='center')\n",
    "axs[1].set_ylabel('Number of UTXOs')\n",
    "axs[1].set_title('Monthly number of UTXOs')\n",
    "axs[1].set_xlabel('Month')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33dfd2",
   "metadata": {},
   "source": [
    "## Deepbit.net graphs parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde3f8c",
   "metadata": {},
   "source": [
    "Preparing the data for the graphs of DeepBit.net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Find the transaction with txId 1883820\n",
    "transaction = transactions[transactions['txId'] == 1883820].merge(outputs, on='txId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0a27a",
   "metadata": {},
   "source": [
    "### 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepbit_graph = nx.MultiDiGraph()\n",
    "\n",
    "# Initially add the unique output addresses to the graph\n",
    "\n",
    "# Aggiungi nodo con tutti gli attributi della riga\n",
    "deepbit_graph.add_node(transaction.iloc[0]['txId'])\n",
    "\n",
    "\n",
    "output_info = outputs[outputs['txId'] >= transaction.iloc[0]['txId']].copy()\n",
    "inputs_info = inputs[inputs['txId'] >= transaction.iloc[0]['txId']].copy()\n",
    "\n",
    "\n",
    "current_transaction = transaction\n",
    "\n",
    "OthersDeepbit =set() # outputs not belonging to Deepbit.net\n",
    "\n",
    "while True:\n",
    "    # Find the inputs for the current transaction\n",
    "    print(f\"Processing transaction {current_transaction.iloc[0]['txId']}...\")\n",
    "    outs = output_info.merge(current_transaction['txId'], on='txId')\n",
    "    \n",
    "    # check if there is a Deepbit.net address\n",
    "    check_deepbit_addr= outs.merge(deepbit_mapped_addresses, on='addressId', how='left', indicator='is_deepbit').rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "    deepbit_addrs = check_deepbit_addr[check_deepbit_addr['is_deepbit'] == 'both']\n",
    "    other_addrs = check_deepbit_addr[check_deepbit_addr['is_deepbit'] == 'left_only']\n",
    "\n",
    "    if len(deepbit_addrs) !=1 :\n",
    "        print(f\"Stop: trovati {len(check_deepbit_addr)} change-outputs in tx {current_transaction.iloc[0]['txId']}\")\n",
    "        break\n",
    "    if not other_addrs.empty:\n",
    "        # Add the outputs that do not belong to Deepbit.net to the set\n",
    "        OthersDeepbit.update(other_addrs['addressId'].unique())\n",
    "        # print(f\"Found {len(OthersDeepbit)} outputs that do not belong to Deepbit.net in tx {current_transaction.iloc[0]['txId']}\")\n",
    "\n",
    "    \n",
    "    # Find the next transaction\n",
    "    next_transactions = inputs_info.merge(deepbit_addrs, on=['prevTxId', 'prevTxpos'])#.rename(columns={'prevTxId':'txId', 'prevTxpos':'position' })\n",
    "    if next_transactions.empty:\n",
    "        print(f\"Stop: la catena della transazione {current_transaction['txId']} si ferma qui.\")\n",
    "        break\n",
    "    # Add the next transaction to the graph\n",
    "\n",
    "    next_tx_id = next_transactions.iloc[0]['txId']\n",
    "\n",
    "    print(f\"Adding transaction {next_tx_id} to the graph\")\n",
    "    deepbit_graph.add_node(next_tx_id)\n",
    "    \n",
    "    current_tx_id = current_transaction.iloc[0]['txId']\n",
    "    change_address = deepbit_addrs.iloc[0]['addressId']  # change address\n",
    "    \n",
    "    # Add an edge from the current transaction to the next transaction\n",
    "    deepbit_graph.add_edge(current_tx_id,next_tx_id, change_address=change_address)\n",
    "    print(f\"Added edge from {current_tx_id} to {next_tx_id} with change address {change_address}\")\n",
    "    # Update the current transaction to the next one\n",
    "    current_transaction = next_transactions\n",
    "\n",
    "print(f\"Total number of transactions in the graph: {deepbit_graph.number_of_nodes()}\")\n",
    "print(f\"Number of OthersDeepbit addresses: {len(OthersDeepbit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d338305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph \n",
    "posizione_circular = nx.spiral_layout(deepbit_graph, equidistant=True, resolution=1)\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx(deepbit_graph, pos= posizione_circular, node_color='lightblue', font_size=7, font_color='black', arrows=True, node_size=100)\n",
    "# Highlight the start node in red\n",
    "nx.draw_networkx_nodes(deepbit_graph, nodelist=[list(deepbit_graph.nodes())[0]], node_color='red', node_size=200, label='Start Node', pos=posizione_circular)\n",
    "\n",
    "nx.draw_networkx_edge_labels(deepbit_graph, pos=posizione_circular, edge_labels={(u, v): d['change_address'] for u, v, d in deepbit_graph.edges(data=True)}, font_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c7f7",
   "metadata": {},
   "source": [
    "# 2)\n",
    "\n",
    "per ogni coppia di transazioni ti, ti+1 della catena calcolare rispettivamente, la differenza tra i timestamp delle due transazioni e tra i valori inviati sui change address e visualizzare le differenze ottenute su un grafico;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txId_chain = pd.Series(list(deepbit_graph.nodes()), name='txId')\n",
    "\n",
    "payment_info = transactions[['timestamp', 'txId']].merge(txId_chain, on='txId').reset_index()\n",
    "\n",
    "# Calcola le differenze tra timestamp consecutivi\n",
    "diff_timestamp = pd.Series(payment_info['timestamp'].diff().shift(-1), name='timestamp_diff')\n",
    "\n",
    "# Calcola le differenze tra gli amount consecutivi sui change address\n",
    "amount_info = outputs.merge(payment_info, on='txId')\n",
    "amount_diff = pd.Series(amount_info['amount'].diff().shift(-1).abs()/ 1e8, name='amount_diff')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Differenze tra timestamp e amount sui change address nella catena DeepBit.net', fontsize=16)\n",
    "\n",
    "# Primo subplot: differenza timestamp\n",
    "axs[0].plot(diff_timestamp.index, diff_timestamp, color='tab:blue', marker='o')\n",
    "axs[0].set_title('Differenza timestamp tra transazioni consecutive')\n",
    "axs[0].set_xlabel('Indice nella catena')\n",
    "axs[0].set_ylabel('Differenza timestamp (ms)')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Secondo subplot: differenza amount\n",
    "axs[1].plot(amount_diff.index, amount_diff, color='tab:orange', marker='x')\n",
    "axs[1].set_title('Differenza amount tra transazioni consecutive')\n",
    "axs[1].set_xlabel('Indice nella catena')\n",
    "axs[1].set_ylabel('Differenza amount (satoshi)')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "# axs[1].ticklabel_format(style='plain', axis='y')  # Disabilita la notazione scientifica sull'asse y\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ecab57",
   "metadata": {},
   "source": [
    "# 3)\n",
    "\n",
    "considerare gli indirizzi in OthersDeepbit, e verificare, effettuando scraping su WalletExplorer, se corrispondono a entità deanonimizzate;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_INTERVAL = 6  # secondi tra le richieste\n",
    "\n",
    "def is_deanonymized(address):\n",
    "    \"\"\"Effettua scraping su Wallet Explorer per ottenere l'hash del wallet dato un address\"\"\"\n",
    "\n",
    "    \n",
    "    url = f\"https://www.walletexplorer.com/?q={address}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(REQUEST_INTERVAL)  # Attendi tra le richieste per evitare blocchi\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=50)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Tentativo 1: Cerca nel div header\n",
    "        main_div = soup.find('div',id='main')\n",
    "        h2_tag = main_div.find('h2') if main_div else None\n",
    "        \n",
    "        if h2_tag:\n",
    "            # Prendi il testo completo\n",
    "            full = h2_tag.get_text(separator=' ').strip()\n",
    "            # Estrai tutte le sottostringhe fra virgolette doppie\n",
    "            quoted = full.split(' ')\n",
    "            # Pulisci gli elementi e verifica che ci sia almeno un secondo elemento\n",
    "            cleaned = [s.strip() for s in quoted]\n",
    "            if len(cleaned) >= 2:\n",
    "                wallet_hash = cleaned[2]\n",
    "                if wallet_hash[0] != '[' and wallet_hash[-1] != ']':\n",
    "                    # Rimuovi le parentesi quadre\n",
    "                    return False\n",
    "\n",
    "                return True\n",
    "        \n",
    "        # Se non trovato\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante lo scraping per {address}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "otheraddresses_mapped = mapping.merge(pd.Series(list(OthersDeepbit), name='addressId'), on='addressId')['hash']\n",
    "\n",
    "\n",
    "deanonymized_addresses = set()\n",
    "for address in otheraddresses_mapped:\n",
    "    res = is_deanonymized(address)\n",
    "    if res is not None and res:\n",
    "        deanonymized_addresses.add(address)\n",
    "        print(f\"Address {address} is deanonymized.\")\n",
    "\n",
    "print(f\"Total deanonymized addresses: {len(deanonymized_addresses)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24b603",
   "metadata": {},
   "source": [
    "## DiceOnCrack.com gambling service analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if diceoncrack_wallet_addresses is None:\n",
    "    #in case of error in the scraping process, we can use a predefined list of wallet addresses\n",
    "diceoncrack_wallet_addresses = pd.Series([\n",
    "    \"12TaAbLWBNKB1NLYH92CPnC1DizQoNK6FN\",\n",
    "    \"1CRACkbiJSxfDaLNEoaNsHjNtU4KttwHyo\",\n",
    "    \"1CRACKafkXsQzUYmu2fUM3j9c2y4yDhvfh\",\n",
    "    \"1CRACKLiwFrZbAQz1yb9w22onHCMLbiMTY\",\n",
    "    \"12tAabLFLxvUzC5KuX7VKMM8bYRncbQ84E\",\n",
    "    \"1CrAcKt3HE8LNsx4KKDvjqLvcr373wg5ke\",\n",
    "    \"1AVFypuG2jUrYzjZa69C7hK59XkWUwvK1m\",\n",
    "    \"1CRACK25QvpVdcEmPZVD5ixtf99cMF9stg\",\n",
    "    \"1CracksLRtQMcTF4HXNrvPzRgvz7Qr6wNd\",\n",
    "    \"13TAabLHjNzwg8Mj7XYn76FuVAqj32s8EM\",\n",
    "    \"1CrAckQppdcfiiw4XzpsKrZrf9eDvUok9C\",\n",
    "    \"19TAABLQTLxgWHTdm7yNJNstgeQFgxTP4f\",\n",
    "    \"14TAAbLiw2QLuRJCGQ3iETYg3vcpweZkTE\",\n",
    "    \"15TaABLmhxiRQ9DTX6ZcZ9S9RknVZmP5jX\",\n",
    "    \"1tAabLBcZLVL7md9nAnvGMCYdbvq4UVZV\",\n",
    "    \"1PipEaL8yRS8n93mUS16wT5SNDiMrMutv5\",\n",
    "    \"1PipemCUjxq9LKww7CaLWUMeGVZL3bD3VM\",\n",
    "    \"1LQXotaEjfmerkwrGB3dHnheujo7sng6vA\",\n",
    "    \"1PipeBMryPGnN3Ms3HfnNjetCS4THmkpkS\",\n",
    "    \"1PipeZHgQXcjAYsUQ4WRXyKZn1X3sJNrpk\",\n",
    "    \"1PipePezjvE7vBukPyDUkhHEF54qK1nkeu\",\n",
    "    \"1Q44t4knYY3PsQZUFAejhd7Wot79ecHe8e\",\n",
    "    \"1F4VXTQRzVQfLaGEWcf697xj1g2cKqPire\",\n",
    "    \"1Pipeb5iNYmURifrxPZfvwHsTiw9rEb2iu\",\n",
    "    \"1PipeZofhJv1hxsxCadEeG1vHAK87f23LE\",\n",
    "    \"17ZmFwCULT44K25kWDeYbHiGaJCrWtytjx\",\n",
    "    \"13encD1Yagh8M6a9Wgb3YJxKHrHqXnYi8y\",\n",
    "    \"1GD2EiVa1rbbXcmFceyM47YN16fzVwn9j\"\n",
    "],name='hash')\n",
    "\n",
    "\n",
    "diceoncrack_mapped_addresses = map_wallet_addresses(diceoncrack_wallet_addresses, mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2a53a",
   "metadata": {},
   "source": [
    "### 1) Find the transaction of DiceOnCrack.com which are the transactions that have at least one input or output address of DiceOnCrack.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the transactions related to DiceOnCrack.com\n",
    "diceoncrack_transactions = find_wallet_transactions(diceoncrack_mapped_addresses, transactions, inputs, outputs)\n",
    "# Order by blockId in ascending order\n",
    "diceoncrack_transactions = diceoncrack_transactions.sort_values(by='blockId', ascending=True)\n",
    "print(f\"Number of transactions related to DiceOnCrack.com: {len(diceoncrack_transactions)}\")\n",
    "# DEBUG\n",
    "# print(diceoncrack_transactions.head(20))\n",
    "# print(diceoncrack_transactions.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47b271",
   "metadata": {},
   "source": [
    "### 2) Group transactions by block height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_diceoncrack_transactions_by_block(diceoncrack_transactions):\n",
    "    \"\"\"Groups DiceOnCrack.com transactions by blockId.\"\"\"\n",
    "    \n",
    "    #find the group of transactions that have the same blockId\n",
    "    grouped_transactions = diceoncrack_transactions.groupby('blockId')\n",
    "    return grouped_transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5083dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "# Set the period : example starting from 26/12/2012\n",
    "WALLET_SCRAPING_CACHE = {}\n",
    "if WALLET_SCRAPING_CACHE is None or len(WALLET_SCRAPING_CACHE) == 0:\n",
    "    WALLET_SCRAPING_CACHE = pd.read_csv('wallet_scraping_cache.csv', index_col=0).to_dict(orient='index')\n",
    "\n",
    "\n",
    "REQUEST_INTERVAL = 7  # secondi tra le richieste\n",
    "\n",
    "def get_wallet_hash(address):\n",
    "    \"\"\"Effettua scraping su Wallet Explorer per ottenere l'hash del wallet dato un address\"\"\"\n",
    "    if address in WALLET_SCRAPING_CACHE:\n",
    "        return WALLET_SCRAPING_CACHE[address]\n",
    "    \n",
    "    url = f\"https://www.walletexplorer.com/?q={address}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(REQUEST_INTERVAL)  # Attendi tra le richieste per evitare blocchi\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=50)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        main_div = soup.find('div',id='main')\n",
    "        h2_tag = main_div.find('h2') if main_div else None\n",
    "        \n",
    "        if h2_tag:\n",
    "            # Prendi il testo completo\n",
    "            full = h2_tag.get_text(separator=' ').strip()\n",
    "            # Estrai tutte le sottostringhe fra virgolette doppie\n",
    "            quoted = full.split(' ')\n",
    "            # Pulisci gli elementi e verifica che ci sia almeno un secondo elemento\n",
    "            cleaned = [s.strip() for s in quoted]\n",
    "            if len(cleaned) >= 2:\n",
    "                wallet_hash = cleaned[2]\n",
    "                if wallet_hash[0] == '[' and wallet_hash[-1] == ']':\n",
    "                    # Rimuovi le parentesi quadre\n",
    "                    wallet_hash = wallet_hash[1:-1]\n",
    "                # cache e rate‑limit\n",
    "                # print(f\"Wallet hash trovato per {address}: {wallet_hash}\")\n",
    "                # get_wallet_hash.last_request_time = current_time\n",
    "                WALLET_SCRAPING_CACHE[address] = wallet_hash\n",
    "                return wallet_hash\n",
    "        \n",
    "        # Se non trovato\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante lo scraping per {address}: {str(e)}\")\n",
    "        return None\n",
    "    # finally:\n",
    "        # get_wallet_hash.last_request_time = time.time()\n",
    "\n",
    "def cluster_by_wallet(group_info, diceoncrack_wallet_ids):\n",
    "\n",
    "    grouped_infor_by_txid = group_info.groupby('txId')\n",
    "\n",
    "    cluster_info =[]\n",
    "\n",
    "    inputs_current_transactions= inputs.merge(group_info,on ='txId')\n",
    "    renamed_outputs = outputs.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "    tmp = inputs_current_transactions.merge(renamed_outputs, on=['prevTxId', 'prevTxpos'])\n",
    "    \n",
    "\n",
    "    for tx_id, group in grouped_infor_by_txid:\n",
    "\n",
    "        inputs_mapped_address= tmp[tmp['txId'] == tx_id]['addressId']\n",
    "        if inputs_mapped_address.empty:\n",
    "            print(f\"No inputs found for txId {tx_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing txId {tx_id} with {len(inputs_mapped_address)} input addresses.\")\n",
    "\n",
    "        # Exclude the cluster if any address belongs to DiceOnCrack\n",
    "        # a =inputs_mapped_address # DEBUG\n",
    "        inputs_mapped_address = inputs_mapped_address[inputs_mapped_address.isin(diceoncrack_wallet_ids) == False]\n",
    "        if inputs_mapped_address.empty:\n",
    "            print(f\"Skipping txId {tx_id} due to no valid addresses.\")\n",
    "            continue\n",
    "\n",
    "        input_address = mapping.merge(inputs_mapped_address, on='addressId')['hash']\n",
    "\n",
    "        print(\"\\nIndirizzi da controllare:\", input_address.tolist(),\"\\n\")\n",
    "        cluster_wallet = get_wallet_hash(input_address.iloc[0])  # Prendi il wallet hash del primo address\n",
    "        if len(input_address) > 1:\n",
    "            for addr in input_address:\n",
    "                try:\n",
    "                    wallet_hash = get_wallet_hash(addr)\n",
    "                    if wallet_hash != cluster_wallet:\n",
    "                        print(f\"Cluster skipped for txId {tx_id} due to different wallet hashes.\")\n",
    "                        cluster_wallet = None\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while getting wallet hash for address {addr}: {e}\")\n",
    "                    break\n",
    "        \n",
    "        if cluster_wallet is None:\n",
    "            # print(f\"Skipping cluster for txId {tx_id} due to inconsistent wallet hashes.\")\n",
    "            continue\n",
    "        else:\n",
    "            # print(f\"Cluster for txId {tx_id} belongs to wallet: {cluster_wallet}\")\n",
    "            # save the cluster information\n",
    "            cluster_info.append({\n",
    "                'txId': tx_id,\n",
    "                'wallet': cluster_wallet,\n",
    "                'num_addresses': len(input_address)})\n",
    "\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    cluster_wallets = pd.DataFrame(cluster_info, columns=['txId', 'wallet','num_addresses']) if len(cluster_info) > 0 else pd.DataFrame()\n",
    "    # print(cluster_wallets.info())\n",
    "    # print(cluster_wallets.head(5))\n",
    "    # print(\"------------------------------------\\n\")\n",
    "    # print(f\"Number of clusters found: {len(cluster_wallets)}\")\n",
    "    # print(cluster_wallets.head(20))\n",
    "    return cluster_wallets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def create_address_clusters(group_info, diceoncrack_wallet_ids, inputs_df, mapping_df):\n",
    "    \"\"\"\n",
    "    Crea cluster di transazioni per ogni indirizzo di input\n",
    "    \n",
    "    Args:\n",
    "        group_info: DataFrame con le transazioni di interesse\n",
    "        diceoncrack_wallet_ids: Lista di addressId da escludere\n",
    "        inputs_df: DataFrame degli input\n",
    "        mapping_df: DataFrame di mappatura addressId -> hash\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con cluster di transazioni per indirizzo\n",
    "    \"\"\"\n",
    "    # Filtra solo le transazioni di interesse\n",
    "    inputs_current_transactions= inputs.merge(group_info,on ='txId')\n",
    "    renamed_outputs = outputs.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "    relevant_inputs = inputs_current_transactions.merge(renamed_outputs, on=['prevTxId', 'prevTxpos'])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Unisci con la mappatura per ottenere gli hash degli indirizzi\n",
    "    inputs_with_hash = relevant_inputs.merge(mapping_df,on='addressId')\n",
    "    \n",
    "    # Filtra gli indirizzi esclusi\n",
    "    filtered_inputs = inputs_with_hash[\n",
    "        ~inputs_with_hash['addressId'].isin(diceoncrack_wallet_ids)\n",
    "    ]\n",
    "    if filtered_inputs.empty:\n",
    "        print(\"No valid addresses found after filtering. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['address', 'wallet', 'transactions', 'num_transactions'])\n",
    "    \n",
    "    # Dizionari per memorizzare i cluster\n",
    "    address_to_transactions = defaultdict(set)\n",
    "    address_to_wallet = {}\n",
    "    \n",
    "    # Processa ogni indirizzo univoco\n",
    "    unique_addresses = filtered_inputs['hash'].unique()\n",
    "    for address in unique_addresses:\n",
    "        # Ottieni l'hash del wallet\n",
    "        wallet_hash = get_wallet_hash(address)\n",
    "        if wallet_hash:\n",
    "            address_to_wallet[address] = wallet_hash\n",
    "            \n",
    "            # Trova tutte le transazioni per questo indirizzo\n",
    "            address_txs = filtered_inputs[filtered_inputs['hash'] == address]['txId']\n",
    "            address_to_transactions[address].update(address_txs)\n",
    "    \n",
    "    # Costruisci il risultato\n",
    "    cluster_data = []\n",
    "    for address, transactions in address_to_transactions.items():\n",
    "        cluster_data.append({\n",
    "            'address': address,\n",
    "            'wallet': address_to_wallet[address],\n",
    "            'transactions': transactions,\n",
    "            'num_transactions': len(transactions)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_period = pd.to_datetime('2012-12-26')\n",
    "\n",
    "diceoncrack_transactions = diceoncrack_transactions[\n",
    "    diceoncrack_transactions['timestamp'] >= starting_period\n",
    "]\n",
    "\n",
    "print(f\"Number of transactions after filtering by date: {len(diceoncrack_transactions)}\")\n",
    "# print(diceoncrack_transactions.head(5))\n",
    "\n",
    "groups = group_diceoncrack_transactions_by_block(diceoncrack_transactions)\n",
    "print(f\"Number of blocks with DiceOnCrack.com transactions: {len(groups)}\")\n",
    "# # DEBUG\n",
    "# print(groups.head(20))\n",
    "# print(groups.head(5))\n",
    "\n",
    "# Esempio di utilizzo\n",
    "frames = []\n",
    "\n",
    "for block_id, group in groups:\n",
    "    tx_ids = group['txId']\n",
    "    \n",
    "    # # Ottimizzazione: calcola tutto in vettori\n",
    "    # clusters = cluster_by_wallet(\n",
    "    #     group_info=group,\n",
    "    #     diceoncrack_wallet_ids=diceoncrack_mapped_addresses\n",
    "    # )\n",
    "    # print(\"Clusters\\n\", clusters.head(15))  # DEBUG\n",
    "\n",
    "    # if len(clusters) > 0:\n",
    "    #     clusters['blockId'] = block_id\n",
    "    #     frames.append(clusters)\n",
    "    clusters = create_address_clusters(\n",
    "        group_info=group,\n",
    "        diceoncrack_wallet_ids=diceoncrack_mapped_addresses,\n",
    "        inputs_df=inputs,\n",
    "        mapping_df=mapping\n",
    "    )\n",
    "    if clusters.empty:\n",
    "        print(f\"No clusters found for block {block_id}.\")\n",
    "        continue\n",
    "    print(f\"Number of transaction clusters found in block {block_id}: {len(clusters)}\")\n",
    "    # print(clusters.head(15))  # DEBUG\n",
    "    frames.append(clusters)\n",
    "\n",
    "# all_clusters['blockId'] = groups['blockId']\n",
    "\n",
    "all_clusters = pd.concat(frames, ignore_index=True, axis=0) if frames else pd.DataFrame()\n",
    "\n",
    "print(all_clusters.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ccf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_clusters = pd.concat(frames, ignore_index=True, axis=0) if frames else pd.DataFrame()\n",
    "\n",
    "# print(all_clusters.head(5))\n",
    "\n",
    "# agroupby_wallet = all_clusters.groupby('wallet').agg({\n",
    "#     'transactions': lambda x: set().union(*x),\n",
    "#     'num_transactions': 'sum'\n",
    "# }).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_clusters.empty:\n",
    "    groupby_block= all_clusters['blockId'].unique().sort()\n",
    "\n",
    "    average_cluster_size = all_clusters[all_clusters['num_transactions'] >= 2].groupby('blockId')['num_transactions'].mean().reset_index()\n",
    "    average_cluster_size.rename(columns={'num_transactions': 'average_size'}, inplace=True)\n",
    "    print(f\"Average cluster size for blocks: {average_cluster_size.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For DEBUG purposes\n",
    "## Create a cache for the cluster wallets research\n",
    "# In this case the file save the research of the clusters wallets starting from 27/12/2012\n",
    "try:\n",
    "    all_clusters = pd.concat(frames, ignore_index=True, axis=0) if frames else pd.DataFrame()\n",
    "    all_clusters.to_csv('cluster_results_26_12_2012.csv', \n",
    "                       index=False, \n",
    "                       encoding='utf-8')  # -sig per compatibilità Excel\n",
    "    \n",
    "    WALLET_SCRAPING_CACHE_pd = pd.DataFrame.from_dict(WALLET_SCRAPING_CACHE, orient='index', columns=['hash']).reset_index()\n",
    "    WALLET_SCRAPING_CACHE_pd.to_csv('wallet_scraping_cache.csv', \n",
    "                                    index=False, \n",
    "                                    encoding='utf-8')  # -sig per compatibilità Excel\n",
    "    print(\"File salvato correttamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel salvataggio: {e}\")\n",
    "\n",
    "#Load the clusters from the file if it exists\n",
    "if all_clusters.empty or all_clusters is None:\n",
    "    all_clusters = pd.read_csv('cluster_results_26_12_2012.csv', \n",
    "                               encoding='utf-8')  # -sig per compatibilità Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6252705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3 Calcola la dimensione media dei cluster per ogni blocco, considerando solo cluster con almeno 2 indirizzi\n",
    "# if not all_clusters.empty:\n",
    "#     # Filtra prima i cluster con almeno 2 indirizzi\n",
    "#     filtered = all_clusters[all_clusters['num_addresses'] >= 2]\n",
    "    \n",
    "#     # Calcola la media per blocco\n",
    "#     avg_cluster_size_per_block = filtered.groupby('blockId')['num_addresses'].mean().reset_index()\n",
    "\n",
    "#     if not avg_cluster_size_per_block.empty:\n",
    "#         print(\"Dimensione media dei cluster (>=2) per blocco:\")\n",
    "#         # print(avg_cluster_size_per_block)\n",
    "        \n",
    "#         # Grafico\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         avg_cluster_size_per_block.plot(kind='bar', x='blockId', y='num_addresses', legend=False)\n",
    "#         plt.title('Dimensione media dei cluster (>=2) per blocco')\n",
    "#         plt.xlabel('blockId')\n",
    "#         plt.ylabel('Dimensione media cluster')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"Nessun cluster con almeno 2 indirizzi trovato.\")\n",
    "# else:\n",
    "#     print(\"Nessun cluster disponibile per il calcolo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed96c54",
   "metadata": {},
   "source": [
    "#### 4) Discussion about\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
