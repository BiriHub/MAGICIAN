{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690fec77",
   "metadata": {},
   "source": [
    "## Setting up the environment for the project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0471610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie necessarie\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987235d5",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.walletexplorer.com\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e258d",
   "metadata": {},
   "source": [
    "### Check the robot.txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to check if a URL is allowed by robots.txt\n",
    "# def is_allowed(url):\n",
    "#     return rp.can_fetch('*', url)\n",
    "# # Function to scrape a URL if allowed by robots.txt\n",
    "# def scrape_url(url):\n",
    "#     if is_allowed(url):\n",
    "#         response = requests.get(url)\n",
    "#         # Process the response\n",
    "#         print(response.status_code)\n",
    "#         print(response.text)\n",
    "#     else:\n",
    "#         print(f\"Scraping blocked by robots.txt: {url}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Parse the robots.txt file \n",
    "# rp = urllib.robotparser.RobotFileParser()\n",
    "# rp.set_url(BASE_URL + '/robots.txt')\n",
    "# rp.read()\n",
    "\n",
    "# if not rp.mtime():\n",
    "#    print(\"robots.txt could not be read or is not present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a943e",
   "metadata": {},
   "source": [
    "## Extracting DeepBit.net and DiceOnCrack.com wallet addresses\n",
    "\n",
    "I create a small pipeline to scrape the walletexplorer website and then extract the wallet addresses from the pages of the two websites by using two functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walletexplorer_page():\n",
    "    ''' Scrape the main page of WalletExplorer to find the search form '''\n",
    "    try:\n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server\n",
    "        print(\"Accessing WalletExplorer main page...\")\n",
    "        main_walletexplore_page = requests.get(BASE_URL, headers=HEADERS)\n",
    "        main_walletexplore_page.raise_for_status()\n",
    "        print(\"WalletExplorer has been successfully accessed\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error while accessing WalletExplorer:\", e)\n",
    "        return None\n",
    "    print(main_walletexplore_page.status_code)\n",
    "    return main_walletexplore_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce59b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wallet_address(html_page,service_name):\n",
    "    \n",
    "    # Search the form in the page\n",
    "    soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "    search_form = soup.find('form', {'class':'main'})\n",
    "\n",
    "    action_form = search_form.get('action')\n",
    "\n",
    "    target_url = BASE_URL + action_form if action_form.startswith('/') else action_form\n",
    "\n",
    "\n",
    "    # Open search page regarding 'service_name' and open the wallet addresses page\n",
    "    try: \n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server\n",
    "        print(f\"Accessing the search page for '{service_name}'...\")\n",
    "\n",
    "        search_page = requests.get(target_url, headers=HEADERS, params={'wallet' :service_name})\n",
    "        search_page.raise_for_status()\n",
    "        print(f'Search page for \"{service_name}\" has been successfully accessed')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error while accessing the search page:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "    ## Scrape the search results and extract the wallet addresses of 'service_name'\n",
    "    soup = BeautifulSoup(search_page.text, 'html.parser')\n",
    "\n",
    "    # Find the url of the wallet addresses page\n",
    "    span = soup.find('span', {'class': 'showother'})\n",
    "\n",
    "    wallet_link = span.find('a').get('href')\n",
    "    wallets_url = BASE_URL + wallet_link # create the full URL for the wallet addresses page\n",
    "    try:\n",
    "        time.sleep(5)  # Pause for 5 seconds to avoid overwhelming the server        \n",
    "        wallet_addr_page = requests.get(wallets_url, headers=HEADERS)\n",
    "        wallet_addr_page.raise_for_status()\n",
    "        print(f\"Wallet addresses page for '{service_name}' has been successfully accessed\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while accessing the wallets page: {e}\")\n",
    "        return None     \n",
    "\n",
    "    # Scrape the wallet addresses page extracting the information from the table\n",
    "    soup = BeautifulSoup(wallet_addr_page.text, 'html.parser')\n",
    "\n",
    "    # Save the wallet address of 'service_name'\n",
    "    wallet_addresses = []\n",
    "\n",
    "    # Find the table containing the wallet addresses\n",
    "    wallet_table = soup.find('table')\n",
    "\n",
    "    for row in wallet_table.find_all('tr'):\n",
    "        col = row.find('td')\n",
    "        if col and col.find('a', href=True):\n",
    "            addr = col.find('a')\n",
    "            wallet_addresses.append(addr.text.strip())\n",
    "    return wallet_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abacc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepbit_service = \"DeepBit.net\"\n",
    "diceoncrack_service = \"DiceOnCrack.com\"\n",
    "\n",
    "# Open the main page of WalletExplorer\n",
    "\n",
    "main_walletexplore_page = get_walletexplorer_page()\n",
    "if main_walletexplore_page is None:\n",
    "    print(\"Failed to retrieve the main WalletExplorer page.\")\n",
    "else:\n",
    "    # Get the wallet addresses for DeepBit.net\n",
    "    print(f\"Searching for wallet addresses of {deepbit_service}...\")\n",
    "    deepbit_wallet_addresses = get_wallet_address(main_walletexplore_page, deepbit_service)\n",
    "    if deepbit_wallet_addresses is None:\n",
    "        print(f\"Failed to retrieve wallet addresses for {deepbit_service}.\")\n",
    "    \n",
    "    # Get the wallet addresses for DiceOnCrack.com\n",
    "    print(f\"Searching for wallet addresses of {diceoncrack_service}...\")\n",
    "    diceoncrack_wallet_addresses = get_wallet_address(main_walletexplore_page, diceoncrack_service)\n",
    "    if diceoncrack_wallet_addresses is None:\n",
    "        print(f\"Failed to retrieve wallet addresses for {diceoncrack_service}.\")\n",
    "#Print the results\n",
    "print(f\"DeepBit.net wallet addresses: {deepbit_wallet_addresses}\")\n",
    "print(f\"DiceOnCrack.com wallet addresses: {diceoncrack_wallet_addresses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe0786",
   "metadata": {},
   "source": [
    "## Deepbit.net's mining pool analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d87da",
   "metadata": {},
   "source": [
    "## 1. Deepbit.net's mined block distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9547807",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db954734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_wallet_addresses(wallet_addresses, mapping_df):\n",
    "    \"\"\"\n",
    "    Mappa una lista di wallet addresses ai rispettivi addressId nel dataset di mapping.\n",
    "    Restituisce un set di addressId trovati e stampa eventuali indirizzi non trovati.\n",
    "    \"\"\"\n",
    "    mapped_addresses = set()\n",
    "    for address in wallet_addresses:\n",
    "        row = mapping_df[mapping_df['hash'] == address]\n",
    "        if not row.empty:\n",
    "            mapped_addresses.add(row['addressId'].values[0])\n",
    "        else:\n",
    "            print(f\"No mapping found for address: {address}\")\n",
    "    print(f\"Mapped addresses : {mapped_addresses}\")\n",
    "    return mapped_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Identify the mapping between wallet addresses on the dataset\n",
    "mapping = pd.read_csv('mapping.csv', engine='pyarrow', header=None)\n",
    "mapping.columns = ['hash', 'addressId']\n",
    "\n",
    "if mapping is None:\n",
    "    print(\"Failed to retrieve the mapping dataset.\")\n",
    "    exit(1)\n",
    "\n",
    "deepbit_wallet_addresses = [\"1VayNert3x1KzbpzMGt2qdqrAThiRovi8\",\"13NGmRF2SVRg3aKdGNVhXLmhA1JT9p87a8\"]\n",
    "\n",
    "deepbit_mapped_addresses = map_wallet_addresses(deepbit_wallet_addresses, mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae865e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Find the transaction patterns for the wallet addresses\n",
    "\n",
    "# Load datasets\n",
    "transactions = pd.read_csv('transactions.csv', engine='pyarrow')\n",
    "transactions.columns = ['timestamp', 'blockId', 'txId', 'isCoinbase', 'fee']\n",
    "\n",
    "outputs = pd.read_csv('outputs.csv', engine='pyarrow')\n",
    "outputs.columns = ['txId', 'position', 'addressId', 'amount', 'scripttype']\n",
    "\n",
    "inputs = pd.read_csv('inputs.csv', engine='pyarrow')\n",
    "inputs.columns = ['txId', 'prevTxId', 'prevTxpos']\n",
    "\n",
    "# 1. Identify Deepbit addresses (assuming deepbit_mapped_addresses is predefined)\n",
    "# deepbit_mapped_addresses = [...] \n",
    "\n",
    "# 2. Find all transactions that have AT LEAST ONE output to a Deepbit address\n",
    "deepbit_txs = outputs.loc[outputs['addressId'].isin(deepbit_mapped_addresses), 'txId'].unique()\n",
    "\n",
    "# 3. For these transactions, find INPUTS originating from Coinbase transactions\n",
    "# 3a. Retrieve all inputs of Deepbit transactions\n",
    "inputs_deepbit = inputs[inputs['txId'].isin(deepbit_txs)]\n",
    "\n",
    "# 3b. Filter only inputs coming from Coinbase transactions\n",
    "coinbase_txids = transactions[transactions['isCoinbase'] == 1]['txId']\n",
    "inputs_deepbit = inputs_deepbit[inputs_deepbit['prevTxId'].isin(coinbase_txids)]\n",
    "\n",
    "# 4. Verify that Deepbit transactions spend EXCLUSIVELY Coinbase outputs\n",
    "# 4a. Count all inputs of Deepbit transactions\n",
    "all_inputs_counts = inputs.groupby('txId').size().loc[deepbit_txs].rename('total_inputs').fillna(0)\n",
    "\n",
    "# 4b. Count Coinbase inputs (already filtered in step 3)\n",
    "coinbase_inputs_counts = inputs_deepbit.groupby('txId').size().rename('coinbase_inputs')\n",
    "\n",
    "# 4c. Select only transactions where all inputs are from Coinbase\n",
    "valid_spend = all_inputs_counts.index[\n",
    "    all_inputs_counts == coinbase_inputs_counts.reindex(all_inputs_counts.index, fill_value=0)\n",
    "]\n",
    "\n",
    "# 5. Build final result\n",
    "result = (\n",
    "    inputs_deepbit[inputs_deepbit['txId'].isin(valid_spend)]\n",
    "    .rename(columns={'prevTxId': 'coinbaseTx', 'txId': 'deepbitSpendTx'})\n",
    "    [['coinbaseTx', 'deepbitSpendTx']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "print(\"Number of identified Deepbit transactions:\", len(result))\n",
    "result.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403145c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets prepare the data for Deepbit.net block distribution\n",
    "\n",
    "deepbit_mined_blocks_df = transactions[transactions['txId'].isin(result['coinbaseTx'])].copy()\n",
    "\n",
    "# Assumendo che 'deepbit_mined_blocks_df' abbia una colonna 'timestamp' già convertita in datetime\n",
    "deepbit_mined_blocks_df['timestamp'] = pd.to_datetime(deepbit_mined_blocks_df['timestamp'], unit='s')\n",
    "deepbit_mined_blocks_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# 1. Conteggio giornaliero\n",
    "daily = deepbit_mined_blocks_df.resample('D').size()\n",
    "\n",
    "# 2. Conteggio settimanale\n",
    "weekly = deepbit_mined_blocks_df.resample('W').size()\n",
    "\n",
    "# 3. Conteggio mensile\n",
    "monthly = deepbit_mined_blocks_df.resample('ME').size()\n",
    "\n",
    "# Visualizzazione\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(14, 12), sharex=False)\n",
    "fig.suptitle('Distribuzione dei blocchi minati da Deepbit.net', fontsize=16)\n",
    "\n",
    "axs[0].plot(daily.index, daily.values, label='Giornaliero', color='blue')\n",
    "axs[0].set_title('Blocchi minati - Giornaliero')\n",
    "axs[0].set_ylabel('Blocchi')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "axs[1].plot(weekly.index, weekly.values, label='Settimanale', color='green')\n",
    "axs[1].set_title('Blocchi minati - Settimanale')\n",
    "axs[1].set_ylabel('Blocchi')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "axs[2].plot(monthly.index, monthly.values, label='Mensile', color='orange')\n",
    "axs[2].set_title('Blocchi minati - Mensile')\n",
    "axs[2].set_ylabel('Blocchi')\n",
    "axs[2].set_xlabel('Data')\n",
    "axs[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponiamo che 'result' contenga le coinbaseTx di Deepbit.net (come già ottenuto nel tuo notebook)\n",
    "# E che 'transactions' sia il DataFrame delle transazioni con le colonne ['timestamp', 'blockId', 'txId', 'isCoinbase', 'fee']\n",
    "\n",
    "#Lets prepare the data for Deepbit.net fee distribution\n",
    "# 1. Estrai le transazioni coinbase di Deepbit.net\n",
    "deepbit_coinbase_txs = transactions[transactions['txId'].isin(result['coinbaseTx'])].copy()\n",
    "print(f\"Number of Deepbit.net coinbase transactions: {deepbit_coinbase_txs.columns}\")\n",
    "# 2. Converte il timestamp in datetime\n",
    "deepbit_coinbase_txs['datetime'] = pd.to_datetime(deepbit_coinbase_txs['timestamp'], unit='s')\n",
    "print(deepbit_coinbase_txs.head(20))\n",
    "\n",
    "a = deepbit_coinbase_txs[ deepbit_coinbase_txs['fee'] > 0 ]\n",
    "\n",
    "print(f\"Number of Deepbit.net coinbase transactions with fee > 0: {len(a)}\")\n",
    "\n",
    "total_fee = deepbit_coinbase_txs['fee'].sum()\n",
    "print(f\"Total fee collected by Deepbit.net: {total_fee} satoshis\")\n",
    "\n",
    "fee_per_block = deepbit_coinbase_txs.groupby('blockId')['fee'].sum()\n",
    "\n",
    "fee_per_block.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ce2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO DA testare\n",
    "\n",
    "\n",
    "def calculate_deepbit_utxo_monthly(transactions, inputs, outputs, deepbit_address_ids):\n",
    "    \"\"\"Calculates monthly UTXO for Deepbit addresses\"\"\"\n",
    "    \n",
    "    # Find all Deepbit-related transactions\n",
    "    deepbit_outputs = outputs[outputs['addressId'].isin(deepbit_address_ids)].copy()\n",
    "    deepbit_inputs = inputs.merge(\n",
    "        outputs[['txId', 'position', 'addressId']],\n",
    "        left_on=['prevTxId', 'prevTxpos'],\n",
    "        right_on=['txId', 'position'],\n",
    "        how='inner',\n",
    "        suffixes=('', '_prev')\n",
    "    )\n",
    "    deepbit_inputs = deepbit_inputs[deepbit_inputs['addressId'].isin(deepbit_address_ids)]\n",
    "    \n",
    "    # Add timestamps\n",
    "    deepbit_outputs = deepbit_outputs.merge(\n",
    "        transactions[['txId', 'timestamp']], \n",
    "        on='txId'\n",
    "    )\n",
    "    deepbit_inputs = deepbit_inputs.merge(\n",
    "        transactions[['txId', 'timestamp']], \n",
    "        left_on='txId', \n",
    "        right_on='txId'\n",
    "    )\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    deepbit_outputs['datetime'] = pd.to_datetime(deepbit_outputs['timestamp'], unit='s')\n",
    "    deepbit_inputs['datetime'] = pd.to_datetime(deepbit_inputs['timestamp'], unit='s')\n",
    "    \n",
    "    # Calculate monthly UTXO\n",
    "    all_dates = pd.date_range(\n",
    "        start=min(deepbit_outputs['datetime'].min(), deepbit_inputs['datetime'].min()),\n",
    "        end=max(deepbit_outputs['datetime'].max(), deepbit_inputs['datetime'].max()),\n",
    "        freq='MS'\n",
    "    )\n",
    "    \n",
    "    monthly_utxo = []\n",
    "    for month_start in all_dates:\n",
    "        month_end = month_start + pd.offsets.MonthEnd()\n",
    "        \n",
    "        # UTXO = outputs before month end minus inputs before month end\n",
    "        outputs_before = deepbit_outputs[deepbit_outputs['datetime'] <= month_end]\n",
    "        inputs_before = deepbit_inputs[deepbit_inputs['datetime'] <= month_end]\n",
    "        \n",
    "        spent_outputs = set(zip(inputs_before['prevTxId'], inputs_before['prevTxpos']))\n",
    "        unspent_outputs = outputs_before[\n",
    "            ~outputs_before.apply(lambda x: (x['txId'], x['position']) in spent_outputs, axis=1)\n",
    "        ]\n",
    "        \n",
    "        utxo_amount = unspent_outputs['amount'].sum() / 100000000  # Convert to BTC\n",
    "        monthly_utxo.append({\n",
    "            'month': month_start,\n",
    "            'utxo_btc': utxo_amount,\n",
    "            'utxo_count': len(unspent_outputs)\n",
    "        })\n",
    "    \n",
    "    utxo_df = pd.DataFrame(monthly_utxo)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # UTXO value in BTC\n",
    "    ax1.plot(utxo_df['month'], utxo_df['utxo_btc'], marker='o', linewidth=2, markersize=6)\n",
    "    ax1.set_title('Deepbit.net UTXO Over Time (BTC Value)')\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('UTXO (BTC)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # UTXO count\n",
    "    ax2.plot(utxo_df['month'], utxo_df['utxo_count'], marker='s', linewidth=2, markersize=6, color='orange')\n",
    "    ax2.set_title('Deepbit.net UTXO Count Over Time')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('UTXO Count')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== DEEPBIT UTXO STATISTICS ===\")\n",
    "    print(f\"Maximum UTXO: {utxo_df['utxo_btc'].max():.6f} BTC\")\n",
    "    print(f\"Minimum UTXO: {utxo_df['utxo_btc'].min():.6f} BTC\")\n",
    "    print(f\"Average UTXO: {utxo_df['utxo_btc'].mean():.6f} BTC\")\n",
    "    print(f\"Final UTXO: {utxo_df['utxo_btc'].iloc[-1]:.6f} BTC\")\n",
    "    \n",
    "    return utxo_df\n",
    "\n",
    "# Main function to run all analyses\n",
    "def run_deepbit_analysis(deepbit_wallet_addresses):\n",
    "    \"\"\"Runs all required analyses for Deepbit.net\"\"\"\n",
    "    \n",
    "    print(\"=== STARTING DEEPBIT.NET ANALYSIS ===\\n\")\n",
    "    \n",
    "    # 1. Load dataset\n",
    "    transactions = pd.read_csv('transactions.csv', engine='pyarrow')\n",
    "    transactions.columns = ['timestamp', 'blockId', 'txId', 'isCoinbase', 'fee']\n",
    "    \n",
    "    outputs = pd.read_csv('outputs.csv', engine='pyarrow')\n",
    "    outputs.columns = ['txId', 'position', 'addressId', 'amount', 'scripttype']\n",
    "    \n",
    "    inputs = pd.read_csv('inputs.csv', engine='pyarrow')\n",
    "    inputs.columns = ['txId', 'prevTxId', 'prevTxpos']\n",
    "    \n",
    "    # 2. Get Deepbit address IDs\n",
    "    # Assuming you have a mapping from addresses to IDs\n",
    "    deepbit_address_ids = deepbit_mapped_addresses\n",
    "    \n",
    "    # 3. Identify Deepbit Coinbase transactions\n",
    "    # (Use your previously defined function here)\n",
    "    deepbit_coinbases_df = result\n",
    "    \n",
    "    # Placeholder for analysis results\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # 4. Analyze block distribution\n",
    "    if not deepbit_coinbases_df.empty:\n",
    "        print(\"\\n1. BLOCK DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        analysis_results['block_distribution'] = analyze_deepbit_block_distribution(deepbit_coinbases_df)\n",
    "    \n",
    "    # 5. Analyze fees\n",
    "    print(\"\\n2. FEE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    analysis_results['fee_analysis'] = calculate_deepbit_fees(deepbit_coinbases_df, transactions)\n",
    "    \n",
    "    # 6. Calculate monthly UTXO\n",
    "    print(\"\\n3. MONTHLY UTXO ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    analysis_results['utxo_analysis'] = calculate_deepbit_utxo_monthly(transactions, inputs, outputs, deepbit_address_ids)\n",
    "    \n",
    "    print(\"\\n=== DEEPBIT.NET ANALYSIS COMPLETED ===\")\n",
    "    \n",
    "    return {\n",
    "        'coinbase_transactions': deepbit_coinbases_df,\n",
    "        'analyses': analysis_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24b603",
   "metadata": {},
   "source": [
    "## DiceOnCrack.com gambling service analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if diceoncrack_wallet_addresses is None:\n",
    "    #in case of error in the scraping process, we can use a predefined list of wallet addresses\n",
    "diceoncrack_wallet_addresses = pd.Series([\n",
    "    \"12TaAbLWBNKB1NLYH92CPnC1DizQoNK6FN\",\n",
    "    \"1CRACkbiJSxfDaLNEoaNsHjNtU4KttwHyo\",\n",
    "    \"1CRACKafkXsQzUYmu2fUM3j9c2y4yDhvfh\",\n",
    "    \"1CRACKLiwFrZbAQz1yb9w22onHCMLbiMTY\",\n",
    "    \"12tAabLFLxvUzC5KuX7VKMM8bYRncbQ84E\",\n",
    "    \"1CrAcKt3HE8LNsx4KKDvjqLvcr373wg5ke\",\n",
    "    \"1AVFypuG2jUrYzjZa69C7hK59XkWUwvK1m\",\n",
    "    \"1CRACK25QvpVdcEmPZVD5ixtf99cMF9stg\",\n",
    "    \"1CracksLRtQMcTF4HXNrvPzRgvz7Qr6wNd\",\n",
    "    \"13TAabLHjNzwg8Mj7XYn76FuVAqj32s8EM\",\n",
    "    \"1CrAckQppdcfiiw4XzpsKrZrf9eDvUok9C\",\n",
    "    \"19TAABLQTLxgWHTdm7yNJNstgeQFgxTP4f\",\n",
    "    \"14TAAbLiw2QLuRJCGQ3iETYg3vcpweZkTE\",\n",
    "    \"15TaABLmhxiRQ9DTX6ZcZ9S9RknVZmP5jX\",\n",
    "    \"1tAabLBcZLVL7md9nAnvGMCYdbvq4UVZV\",\n",
    "    \"1PipEaL8yRS8n93mUS16wT5SNDiMrMutv5\",\n",
    "    \"1PipemCUjxq9LKww7CaLWUMeGVZL3bD3VM\",\n",
    "    \"1LQXotaEjfmerkwrGB3dHnheujo7sng6vA\",\n",
    "    \"1PipeBMryPGnN3Ms3HfnNjetCS4THmkpkS\",\n",
    "    \"1PipeZHgQXcjAYsUQ4WRXyKZn1X3sJNrpk\",\n",
    "    \"1PipePezjvE7vBukPyDUkhHEF54qK1nkeu\",\n",
    "    \"1Q44t4knYY3PsQZUFAejhd7Wot79ecHe8e\",\n",
    "    \"1F4VXTQRzVQfLaGEWcf697xj1g2cKqPire\",\n",
    "    \"1Pipeb5iNYmURifrxPZfvwHsTiw9rEb2iu\",\n",
    "    \"1PipeZofhJv1hxsxCadEeG1vHAK87f23LE\",\n",
    "    \"17ZmFwCULT44K25kWDeYbHiGaJCrWtytjx\",\n",
    "    \"13encD1Yagh8M6a9Wgb3YJxKHrHqXnYi8y\",\n",
    "    \"1GD2EiVa1rbbXcmFceyM47YN16fzVwn9j\"\n",
    "],name='hash')\n",
    "\n",
    "\n",
    "diceoncrack_mapped_addresses = map_wallet_addresses(diceoncrack_wallet_addresses, mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2a53a",
   "metadata": {},
   "source": [
    "### 1) Find the transaction of DiceOnCrack.com which are the transactions that have at least one input or output address of DiceOnCrack.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diceoncrack_transactions(diceoncrack_mapped_addresses, transactions, inputs, outputs):\n",
    "    \"\"\"Finds all transactions related to DiceOnCrack.com addresses.\"\"\"\n",
    "    # Find all outputs that are related to DiceOnCrack addresses\n",
    "    diceoncrack_output_transactions = outputs[outputs['addressId'].isin(diceoncrack_mapped_addresses)]\n",
    "\n",
    "    # Find all inputs information (address, amount, ...)\n",
    "    renamed_outputs= outputs.rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "\n",
    "    inputs_info = inputs.merge(renamed_outputs,on=['prevTxId', 'prevTxpos'])\n",
    "\n",
    "    # Find all inputs that are related to DiceOnCrack addresses\n",
    "    diceoncrack_inputs_transactions = inputs_info[inputs_info['addressId'].isin(diceoncrack_mapped_addresses)]\n",
    "\n",
    "    # DEBUG\n",
    "    # print(diceoncrack_output_transactions.head(5))\n",
    "    # print(\"---\")\n",
    "    # print(diceoncrack_inputs_transactions.head(5))\n",
    "\n",
    "\n",
    "    # Find all transactions that have at least one output address of DiceOnCrack.com\n",
    "    df_union = pd.concat([diceoncrack_output_transactions['txId'], diceoncrack_inputs_transactions['txId']],axis=0, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    diceoncrack_transactions = transactions[transactions['txId'].isin(df_union)].drop_duplicates()\n",
    "\n",
    "    # Order by blockId in ascending order\n",
    "    diceoncrack_transactions = diceoncrack_transactions.sort_values(by='blockId', ascending=True)\n",
    "    return diceoncrack_transactions\n",
    "\n",
    "# Find the transactions related to DiceOnCrack.com\n",
    "diceoncrack_transactions = find_diceoncrack_transactions(diceoncrack_mapped_addresses, transactions, inputs, outputs)\n",
    "print(f\"Number of transactions related to DiceOnCrack.com: {len(diceoncrack_transactions)}\")\n",
    "# DEBUG\n",
    "# print(diceoncrack_transactions.head(20))\n",
    "# print(diceoncrack_transactions.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47b271",
   "metadata": {},
   "source": [
    "### 2) Group transactions by block height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_diceoncrack_transactions_by_block(diceoncrack_transactions):\n",
    "    \"\"\"Groups DiceOnCrack.com transactions by blockId.\"\"\"\n",
    "    \n",
    "    #find the group of transactions that have the same blockId\n",
    "    grouped_transactions = diceoncrack_transactions.groupby('blockId')\n",
    "    return grouped_transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5083dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Cache globale per memorizzare address -> wallet\n",
    "address_to_wallet_cache = {}\n",
    "REQUEST_INTERVAL = 6  # secondi tra le richieste\n",
    "\n",
    "def get_wallet_hash(address):\n",
    "    \"\"\"Effettua scraping su Wallet Explorer per ottenere l'hash del wallet dato un address\"\"\"\n",
    "\n",
    "    \n",
    "    url = f\"https://www.walletexplorer.com/?q={address}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(REQUEST_INTERVAL)  # Attendi tra le richieste per evitare blocchi\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Tentativo 1: Cerca nel div header\n",
    "        main_div = soup.find('div',id='main')\n",
    "        h2_tag = main_div.find('h2') if main_div else None\n",
    "        \n",
    "        if h2_tag:\n",
    "            # Prendi il testo completo\n",
    "            full = h2_tag.get_text(separator=' ').strip()\n",
    "            # Estrai tutte le sottostringhe fra virgolette doppie\n",
    "            quoted = full.split(' ')\n",
    "            # Pulisci gli elementi e verifica che ci sia almeno un secondo elemento\n",
    "            cleaned = [s.strip() for s in quoted]\n",
    "            if len(cleaned) >= 2:\n",
    "                wallet_hash = cleaned[2]\n",
    "                if wallet_hash[0] == '[' and wallet_hash[-1] == ']':\n",
    "                    # Rimuovi le parentesi quadre\n",
    "                    wallet_hash = wallet_hash[1:-1]\n",
    "                # cache e rate‑limit\n",
    "                print(f\"Wallet hash trovato per {address}: {wallet_hash}\")\n",
    "                address_to_wallet_cache[address] = wallet_hash\n",
    "                # get_wallet_hash.last_request_time = current_time\n",
    "                return wallet_hash\n",
    "        \n",
    "        # Se non trovato\n",
    "        address_to_wallet_cache[address] = None\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante lo scraping per {address}: {str(e)}\")\n",
    "        address_to_wallet_cache[address] = None\n",
    "        return None\n",
    "    # finally:\n",
    "        # get_wallet_hash.last_request_time = time.time()\n",
    "\n",
    "def cluster_by_wallet(group_info, diceoncrack_wallet_ids):\n",
    "    \"\"\"\n",
    "    Crea cluster di transazioni omogenee per wallet di input\n",
    "    utilizzando operazioni vettoriali di pandas\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_infor_by_txid = group_info.groupby('txId')\n",
    "\n",
    "    cluster_info =[]\n",
    "\n",
    "    \n",
    "    ranged_renamed_outputs = outputs.merge(group_info, on='txId').rename(columns={'txId': 'prevTxId', 'position': 'prevTxpos'})\n",
    "    tmp = inputs.merge(ranged_renamed_outputs, on=['prevTxId', 'prevTxpos'])\n",
    "    \n",
    "\n",
    "    for tx_id, group in grouped_infor_by_txid:\n",
    "\n",
    "        inputs_mapped_address= tmp[tmp['prevTxId'] == tx_id]['addressId']\n",
    "\n",
    "        # Exclude the cluster if any address belongs to DiceOnCrack\n",
    "        inputs_mapped_address = inputs_mapped_address[inputs_mapped_address.isin(diceoncrack_wallet_ids) == False]\n",
    "        if inputs_mapped_address.empty:\n",
    "            print(f\"Skipping txId {tx_id} due to no valid addresses.\")\n",
    "            continue\n",
    "\n",
    "        input_address = mapping.merge(inputs_mapped_address, on='addressId')['hash']\n",
    "\n",
    "\n",
    "        cluster_wallet = get_wallet_hash(input_address.iloc[0])  # Prendi il wallet hash del primo address\n",
    "        for addr in input_address:\n",
    "            try:\n",
    "                wallet_hash = get_wallet_hash(addr)\n",
    "                if wallet_hash != cluster_wallet:\n",
    "                    print(f\"Cluster skipped for txId {tx_id} due to different wallet hashes.\")\n",
    "                    cluster_wallet = None\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error while getting wallet hash for address {addr}: {e}\")\n",
    "                break\n",
    "        \n",
    "        if cluster_wallet is None:\n",
    "            # print(f\"Skipping cluster for txId {tx_id} due to inconsistent wallet hashes.\")\n",
    "            continue\n",
    "        else:\n",
    "            # print(f\"Cluster for txId {tx_id} belongs to wallet: {cluster_wallet}\")\n",
    "            # save the cluster information\n",
    "            cluster_info.append({\n",
    "                'txId': tx_id,\n",
    "                'wallet': cluster_wallet,\n",
    "                'num_addresses': len(input_address)})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    cluster_wallets = pd.DataFrame(cluster_info, columns=['txId', 'wallet'])\n",
    "    print(f\"Number of clusters found: {len(cluster_wallets)}\")\n",
    "    print(cluster_wallets.head(20))\n",
    "    return cluster_wallets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcce1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions after filtering by date: 6\n",
      "Number of blocks with DiceOnCrack.com transactions: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m tx_ids = group[\u001b[33m'\u001b[39m\u001b[33mtxId\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Ottimizzazione: calcola tutto in vettori\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m clusters = \u001b[43mcluster_by_wallet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiceoncrack_wallet_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiceoncrack_mapped_addresses\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Add cluster information to the DataFrame\u001b[39;00m\n\u001b[32m     30\u001b[39m all_clusters = pd.concat([all_clusters, clusters], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mcluster_by_wallet\u001b[39m\u001b[34m(group_info, diceoncrack_wallet_ids)\u001b[39m\n\u001b[32m     65\u001b[39m cluster_info =[]\n\u001b[32m     68\u001b[39m ranged_renamed_outputs = outputs.merge(group_info, on=\u001b[33m'\u001b[39m\u001b[33mtxId\u001b[39m\u001b[33m'\u001b[39m).rename(columns={\u001b[33m'\u001b[39m\u001b[33mtxId\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mprevTxId\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mposition\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mprevTxpos\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m tmp = \u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranged_renamed_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprevTxId\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprevTxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tx_id, group \u001b[38;5;129;01min\u001b[39;00m grouped_infor_by_txid:\n\u001b[32m     74\u001b[39m     inputs_mapped_address= tmp[tmp[\u001b[33m'\u001b[39m\u001b[33mprevTxId\u001b[39m\u001b[33m'\u001b[39m] == tx_id][\u001b[33m'\u001b[39m\u001b[33maddressId\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m result = \u001b[38;5;28mself\u001b[39m._reindex_and_concat(\n\u001b[32m    889\u001b[39m     join_index, left_indexer, right_indexer, copy=copy\n\u001b[32m    890\u001b[39m )\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[39m, in \u001b[36m_MergeOperation._get_join_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m     join_index, right_indexer, left_indexer = _left_join_on_index(\n\u001b[32m   1148\u001b[39m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m.right_join_keys, sort=\u001b[38;5;28mself\u001b[39m.sort\n\u001b[32m   1149\u001b[39m     )\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     (left_indexer, right_indexer) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index:\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.left) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1125\u001b[39m, in \u001b[36m_MergeOperation._get_join_indexers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.how != \u001b[33m\"\u001b[39m\u001b[33masof\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhow\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1759\u001b[39m, in \u001b[36mget_join_indexers\u001b[39m\u001b[34m(left_keys, right_keys, sort, how)\u001b[39m\n\u001b[32m   1757\u001b[39m     _, lidx, ridx = left.join(right, how=how, return_indexers=\u001b[38;5;28;01mTrue\u001b[39;00m, sort=sort)\n\u001b[32m   1758\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m     lidx, ridx = \u001b[43mget_join_indexers_non_unique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[32m   1764\u001b[39m     lidx = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1793\u001b[39m, in \u001b[36mget_join_indexers_non_unique\u001b[39m\u001b[34m(left, right, sort, how)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_join_indexers_non_unique\u001b[39m(\n\u001b[32m   1771\u001b[39m     left: ArrayLike,\n\u001b[32m   1772\u001b[39m     right: ArrayLike,\n\u001b[32m   1773\u001b[39m     sort: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1774\u001b[39m     how: JoinHow = \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1775\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n\u001b[32m   1776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[33;03m    Get join indexers for left and right.\u001b[39;00m\n\u001b[32m   1778\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1791\u001b[39m \u001b[33;03m        Indexer into right.\u001b[39;00m\n\u001b[32m   1792\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     lkey, rkey, count = \u001b[43m_factorize_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1795\u001b[39m         lidx, ridx = libjoin.left_outer_join(lkey, rkey, count, sort=sort)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leona\\Desktop\\Leo\\_University\\TerzoAnno_InformaticaUNIPI\\LabWebScraping\\MAGICIAN\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2560\u001b[39m, in \u001b[36m_factorize_keys\u001b[39m\u001b[34m(lk, rk, sort)\u001b[39m\n\u001b[32m   2553\u001b[39m     rlab = rizer.factorize(\n\u001b[32m   2554\u001b[39m         rk.to_numpy(na_value=\u001b[32m1\u001b[39m, dtype=lk.dtype.numpy_dtype), mask=rk.isna()\n\u001b[32m   2555\u001b[39m     )\n\u001b[32m   2556\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2557\u001b[39m     \u001b[38;5;66;03m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[32m   2558\u001b[39m     \u001b[38;5;66;03m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;66;03m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     llab = \u001b[43mrizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   2561\u001b[39m     rlab = rizer.factorize(rk)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m llab.dtype == np.dtype(np.intp), llab.dtype\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set the period : example starting from 26/12/2012\n",
    "\n",
    "diceoncrack_transactions['timestamp'] = pd.to_datetime(diceoncrack_transactions['timestamp'], unit='s')\n",
    "\n",
    "starting_period = pd.to_datetime('2012-12-26')\n",
    "\n",
    "diceoncrack_transactions = diceoncrack_transactions[diceoncrack_transactions['timestamp'] >= starting_period]\n",
    "\n",
    "print(f\"Number of transactions after filtering by date: {len(diceoncrack_transactions)}\")\n",
    "# print(diceoncrack_transactions.head(5))\n",
    "\n",
    "groups = group_diceoncrack_transactions_by_block(diceoncrack_transactions)\n",
    "print(f\"Number of blocks with DiceOnCrack.com transactions: {len(groups)}\")\n",
    "# # DEBUG\n",
    "# print(groups.head(20))\n",
    "# print(groups.head(5))\n",
    "\n",
    "# Esempio di utilizzo\n",
    "all_clusters = pd.DataFrame(columns=['blockId', 'txId', 'wallet'])\n",
    "\n",
    "for block_id, group in groups:\n",
    "    tx_ids = group['txId']\n",
    "    \n",
    "    # Ottimizzazione: calcola tutto in vettori\n",
    "    clusters = cluster_by_wallet(\n",
    "        group_info=group,\n",
    "        diceoncrack_wallet_ids=diceoncrack_mapped_addresses\n",
    "    )\n",
    "    # Add cluster information to the DataFrame\n",
    "    all_clusters = pd.concat([all_clusters, clusters], ignore_index=True, axis=0)\n",
    "\n",
    "\n",
    "print(all_clusters.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
